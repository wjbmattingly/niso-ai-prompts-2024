# NISO Prompt Engineering Course

This course will introduce students to prompt engineering with large language models, or LLMs. It is designed for students with no coding knowledge. It presumes no knowledge about machine learning or large language models. Because the course focuses on prompt engineering, or the way in which you design and tailor a message to an LLM to perform a specific task, a basic knowledge of machine learning will be helpful. This course will, therefore, bring students up to speed with all the necessary terminology and concepts.

This course will be hands-on, meaning it will be divided between instruction and hands-on lessons where students will apply the material as they learn it. LLMs are stochastic, meaning they can behave unpredictably due to the randomness inherent in them. This can make it challenging to reproduce results. While this may introduce some inconsistencies for students during the course, it is an opportunity to learn. We will discuss the issues and challenges that surface to understand better how and why these issues occur.

When working with students across different operating systems, hardware, and experience, it is challenging to work with open-source software and machine learning models. For this reason, the course will require students to have a subscription to ChatGPT for the duration of the course (2 months). This will ensure that all students are working with the same resources and eliminate potential challenges. That said, this course will devote a week to introducing to the world of open-source machine learning so that students may explore it independently after the course concludes.

Because the field of machine learning is advancing rapidly, this course outline may change between its construction (January 2024) and its use (April-May 2024). Nevertheless, the core structure is expected to remain the same.

## Week 1: Introduction and Machine Learning

In this week, we will do a deep dive into machine learning, how it works, how its used, and its limitations. This week will provide a cursory overview of the essential concepts and terminology.

## Week 2: Large Language Models and Key Concepts

With a foundational understanding of machine learning, students will move into Week 2. Here, we will learn about large language models, the technology behind popular tools, such as ChatGPT.

## Week 3: Beginning Conversations

In Week 3, we will begin learning about prompt engineering. We will learn some best practices. Here, we will apply the concepts we have learned in the first two weeks by learning through in-class exercises the strengths and weakness of certain prompt designs.

## Week 4: Structured Data and Assistants

Over the last year, we have seen numerous advancements in LLMs. Two of these are the output of structured data and what are known as `assistants`. In this week, we will learn about why structured data is important and, most importantly, how to engineer a prompt to produce consistent structured outputs from an LLM.

We will also learn how to design assistants. These are LLMs that are designed to perform a specific task by receiving instructions and, in some cases, data before the user first engages with a model.

## Week 5: Named Entity Recognition with LLMs

In week 5, we shift courses a little bit and begin looking at the real-world applications of LLMs and their limitations. We will focus on performing named entity recognition (NER) with these models. The goal of this week is to provide you with a broad understanding of NER, its importance, and how to generate an NER output with an LLM.

## Week 6: Text Classification with LLMs

Building off of NER with Week 5, we will switch to another real-world application of LLMs, namely text classification. Text classification allows us to classify a document or portions of a document. LLMs are important in this area of applied machine learning due, in part, to their large context windows, or the size of data that they can ingest. However, this is not without limitations. In this week we will learn how to do this and how to address some of the key challenges that surface.

## Week 7: Open Source Language Models

Although this course is structured around the use of ChatGPT, it is vital that students learn about the world of open-source machine learning. This is a thriving community centered around HuggingFace, a machine learning platform similar to GitHub that hosts large models and datasets, and makes them freely available to all in a standard and expected way.

For many, the future of machine learning and LLMs is open-source and for that reason, students should be aware of what is available from the open-source community and how to access it. In this week, we will address both of these things.

## Week 8: Limitations and Potential Solutions

A constant theme throughout the previous seven weeks will be limitations of LLMs. This cannot and should not be ignored. Putting an LLM into production without properly vetting the output has the potential to lead to catastrophic consequences, especially if the data is sensitive in nature, as is the case with many archives around the world. In this week, we will dive more deeply into these limitations and we will learn about some of the potential solutions to those issues.